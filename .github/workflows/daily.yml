name: Daily Lottery Collecting

on:
  schedule:
    - cron: '0 11 * * *'   # 17h
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  fetch-database:
    if: github.event_name == 'push'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: pip install -r requirements/fetch.txt

      - name: Fetching html data
        run: python pipeline/fetch_database.py
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_PORT: ${{ secrets.DB_PORT }}

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: raw-data-fetch
          path: data/raw.json

  crawl_data:
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: pip install -r requirements/crawl.txt

      - name: Fetching html data
        run: python pipeline/crawl_data.py

  extract-data:
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    needs: crawl_data
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: pip install -r requirements/extract.txt

      - name: Extracting info from html data
        run: python pipeline/extract_data.py

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: raw-data-extract
          path: data/raw.json

  merge-artifact:
    runs-on: ubuntu-latest
    needs:
      - fetch-database
      - extract-data
    if: |
      always() &&
      (
        (needs.fetch-database.result == 'success') ||
        (needs.extract-data.result == 'success')
      )
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Download from fetch
        if: ${{ needs.fetch-database && needs.fetch-database.result == 'success' }}
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: raw-data-fetch
          path: data/fetch/

      - name: Download from extract
        if: ${{ needs.extract-data && needs.extract-data.result == 'success' }}
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: raw-data-extract
          path: data/extract/

      - name: List merged folder
        run: ls -R data || true

      - name: Upload merged
        uses: actions/upload-artifact@v4
        with:
          name: raw-data-merged
          path: data/

  transform-data:
    runs-on: ubuntu-latest
    needs: merge-artifact
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: pip install -r requirements/transform.txt

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: raw-data-merged
          path: data/

      - name: Transforming data
        run: python pipeline/transform_data.py

  load-data:
    runs-on: ubuntu-latest
    needs: transform-data
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: pip install -r requirements/load.txt

      - name: Loading data
        run: python pipeline/load_data.py
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASS: ${{ secrets.DB_PASS }}
          DB_PORT: ${{ secrets.DB_PORT }}
